#' @title Visualize binary classification predictions via ROCR ROC curves.
#'
#' @description
#' Plots is generated by calling asROCRPrediction, ROCR's \code{\link[ROCR]{performance}},
#' then ROCR's \code{\link[ROCR]{plot}}.
#'
#' Please these methods for further info.
#'
#' @template arg_plotroc_obj
#' @param meas1 [\code{character(1)}]\cr
#'   Measure on x-axis. Note that this is a measure name from *ROCR* and not from mlr!
#'   Default is \dQuote{tpr}.
#' @param meas2 [\code{character(1)}]\cr
#'   Measure on y-axis. Note that this is a measure name from *ROCR* and not from mlr!
#'   Default is \dQuote{tpr}.
#' @param avg [\code{character(1)}]\cr
#'   How to average results from resampling.
#'   Default is \dQuote{threshold}.
#' @param perf.args [named \code{list}]\cr
#'   Further args passed to ROCR's \code{\link[ROCR]{performance}}.
#'   Usually not needed and meas1 and meas2 are set internally.
#'   Default is none.
#' @param legend.args [named \code{list}]\cr
#'   Further args passed to \code{\link{legend}}.
#'   Default is to display the names / learner ids of \code{obj},
#'   and to draw in \dQuote{bottomright}.
#' @param task.id [\code{character(1)}]\cr
#'   Selected task in \code{\link{BenchmarkResult}} to do plots for, ignored otherwise.
#'   Default is first task.
#' @template ret_inv_null
#' @family roc
#' @family predict
#' @export
#' @examples
#' \dontrun{
#' lrn1 = makeLearner("classif.logreg", predict.type = "prob")
#' lrn2 = makeLearner("classif.rpart", predict.type = "prob")
#' b = benchmark(list(lrn1, lrn2), pid.task)
#' z = plotROCRCurves(b)
#' }
plotROCRCurves = function(obj, meas1 = "tpr", meas2 = "fpr", avg = "threshold",
  perf.args = list(), legend.args = list(), task.id = NULL) {

  # lets not check the value-names from ROCR here. they might be changed behind our back later...
  assertString(meas1)
  assertString(meas2)
  assertString(avg)
  assertList(perf.args, names = "unique")
  assertList(legend.args, names = "unique")
  UseMethod("plotROCRCurves")
}

#' @export
plotROCRCurves.Prediction = function(obj, meas1 = "tpr", meas2 = "fpr", avg = "threshold",
  perf.args = list(), legend.args = list(), task.id = NULL) {

  l = namedList(names = getTaskId(obj), init = obj)
  plotROCRCurves.list(l, meas1, meas2, avg, perf.args, legend.args)
}

#' @export
plotROCRCurves.ResampleResult = function(obj, meas1 = "tpr", meas2 = "fpr", avg = "threshold",
  perf.args = list(), legend.args = list(), task.id = NULL) {

  l = namedList(names = getTaskId(obj), init = obj)
  plotROCRCurves.list(l, meas1, meas2, avg, perf.args, legend.args)
}

#' @export
plotROCRCurves.list = function(obj, meas1 = "tpr", meas2 = "fpr", avg = "threshold",
  perf.args = list(), legend.args = list(), task.id = NULL) {

  assertList(obj, c("Prediction", "ResampleResult"), min.len = 1L)
  k = length(obj)
  cargs = list(measure = meas1, x.measure = meas2)
  cargs = insert(cargs, perf.args)
  rocr.perfs = lapply(obj, function(x) {
    if (inherits(x, "ResampleResult"))
      x = x$pred
    cargs$prediction.obj = asROCRPrediction(x)
    do.call(ROCR::performance, cargs)
  })
  for (i in 1:k) {
    ROCR::plot(rocr.perfs[[i]], avg = avg, add = (i > 1L))
  }
  if (k > 1L) {
    ns = if (inherits(obj, "ResampleResult"))
      extractSubList(obj, "learner.id")#
    else
      names(obj)
    cargs = list(x = "bottomright", legend = ns)
    cargs = insert(cargs, legend.args)
    do.call(legend, cargs)
  }
  invisible(NULL)
}

#' @export
plotROCRCurves.BenchmarkResult = function(obj, meas1 = "tpr", meas2 = "fpr", avg = "threshold",
  perf.args = list(), legend.args = list(),
  task.id = NULL) {

  tids = getBMRTaskIds(obj)
  if (is.null(task.id))
    task.id = tids[1L]
  else
    assertChoice(task.id, tids)
  ps = getBMRPredictions(obj, task.ids = task.id, as.df = FALSE)[[1L]]
  plotROCRCurves.list(ps, meas1, meas2, avg, perf.args, legend.args)
}

