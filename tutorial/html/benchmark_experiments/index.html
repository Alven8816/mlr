<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        
        <link rel="shortcut icon" href="../img/favicon.ico">

        <title>Benchmark Experiments - mlr tutorial</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../css/prettify-1.0.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="../index.html">mlr tutorial</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Basics <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../task/index.html">Tasks</a>
                            </li>
                        
                            <li >
                                <a href="../learner/index.html">Learner</a>
                            </li>
                        
                            <li >
                                <a href="../train/index.html">Train</a>
                            </li>
                        
                            <li >
                                <a href="../predict/index.html">Predict</a>
                            </li>
                        
                            <li >
                                <a href="../performance/index.html">Performance</a>
                            </li>
                        
                            <li >
                                <a href="../resample/index.html">Resampling</a>
                            </li>
                        
                            <li class="active">
                                <a href="index.html">Benchmark Experiments</a>
                            </li>
                        
                            <li >
                                <a href="../parallelization/index.html">Parallelization</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Advanced <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../configureMlr/index.html">Configuration</a>
                            </li>
                        
                            <li >
                                <a href="../wrapper/index.html">Wrapped Learners</a>
                            </li>
                        
                            <li >
                                <a href="../preproc/index.html">Preprocessing</a>
                            </li>
                        
                            <li >
                                <a href="../impute/index.html">Imputations</a>
                            </li>
                        
                            <li >
                                <a href="../bagging/index.html">Bagging</a>
                            </li>
                        
                            <li >
                                <a href="../tune/index.html">Tuning</a>
                            </li>
                        
                            <li >
                                <a href="../feature_selection/index.html">Feature Selection</a>
                            </li>
                        
                            <li >
                                <a href="../cost_sensitive_classif/index.html">Cost-Sensitive Classification</a>
                            </li>
                        
                            <li >
                                <a href="../over_and_undersampling/index.html">Imbalanced Classification Problems</a>
                            </li>
                        
                            <li >
                                <a href="../roc_analysis/index.html">ROC Analysis</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Extend <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../create_learner/index.html">Create Custom Learners</a>
                            </li>
                        
                            <li >
                                <a href="../create_measure/index.html">Create Custom Measures</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Appendix <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../example_tasks/index.html">Example Tasks</a>
                            </li>
                        
                            <li >
                                <a href="../integrated_learners/index.html">Integrated Learners</a>
                            </li>
                        
                            <li >
                                <a href="../measures/index.html">Implemented Performance Measures</a>
                            </li>
                        
                            <li >
                                <a href="../filter_methods/index.html">Integrated Filter Methods</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                </ul>
            

            
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                
                <li >
                    <a rel="next" href="../resample/index.html">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../parallelization/index.html">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                
                <li>
                    <a href="https://github.com/berndbischl/mlr/">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
            
        </div>
    </div>
</div>

        <div class="container">
            <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#benchmark-experiments">Benchmark Experiments</a></li>
        
            <li><a href="#example-two-tasks-and-two-learners">Example: Two tasks and two learners</a></li>
        
            <li><a href="#nested-resampling">Nested resampling</a></li>
        
    
    </ul>
</div></div>
            <div class="col-md-9" role="main">

<h1 id="benchmark-experiments">Benchmark Experiments</h1>
<p>In a benchmark experiment different learning methods are applied to one or several data sets.
The aim is usually to compare and rank the algorithms with respect to one or more
performance measures.</p>
<p>In <a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> a benchmark experiment can be conducted by calling the function <a href="http://www.rdocumentation.org/packages/mlr/functions/benchmark.html">benchmark</a> on a
list of <a href="http://www.rdocumentation.org/packages/mlr/functions/makeLearner.html">Learner</a>s and <a href="http://www.rdocumentation.org/packages/mlr/functions/Task.html">Task</a>s.
Moreover, you can specify which performance measures to calculate and choose a resampling
strategy for each <a href="http://www.rdocumentation.org/packages/mlr/functions/Task.html">Task</a>.</p>
<h3 id="example-two-tasks-and-two-learners">Example: Two tasks and two learners</h3>
<pre class="prettyprint well"><code class="r">## Two learners to be compared
lrns = list(makeLearner(&quot;classif.lda&quot;), makeLearner(&quot;classif.rpart&quot;))

## Two classification tasks
tasks = list(iris.task, sonar.task)

## Use 5-fold cross-validation for both tasks
rdesc = makeResampleDesc(&quot;CV&quot;, iters = 5)

## Conduct the benchmark experiment
res = benchmark(lrns, tasks, rdesc, show.info = FALSE)
res
</code></pre>

<pre class="prettyprint well"><code>#&gt;         task.id    learner.id mmce.test.mean
#&gt; 1  iris-example   classif.lda     0.04000000
#&gt; 2  iris-example classif.rpart     0.06666667
#&gt; 3 Sonar-example   classif.lda     0.23577236
#&gt; 4 Sonar-example classif.rpart     0.26980256
</code></pre>

<p>The above code should be self-explanatory. In the result every row corresponds to one <a href="http://www.rdocumentation.org/packages/mlr/functions/Task.html">Task</a>
set with one <a href="http://www.rdocumentation.org/packages/mlr/functions/makeLearner.html">Learner</a>.
In the printed table the individual tasks and learners are referred to by their IDs.
The entries show the 5-fold cross-validated mean misclassification error (<a href="http://www.rdocumentation.org/packages/mlr/functions/mmce.html">mmce</a>), which
is the default performance measure for classification.</p>
<p>The result is an object of class <a href="http://www.rdocumentation.org/packages/mlr/functions/BenchmarkResult.html">BenchmarkResult</a>.
<a href="http://www.rdocumentation.org/packages/mlr/">mlr</a> provides several accessor functions, follow the naming convention
<code>getBMR&lt;what_to_extract&gt;</code>, that allow to extract e.g. the performances or the predictions
of the learning algorithms under consideration.
Let's have a look at the benchmark result from the example above:</p>
<pre class="prettyprint well"><code class="r">## Individual performances of the 5 cross-validation runs
getBMRPerformances(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.lda
#&gt;   iter       mmce
#&gt; 1    1 0.03333333
#&gt; 2    2 0.00000000
#&gt; 3    3 0.00000000
#&gt; 4    4 0.00000000
#&gt; 5    5 0.16666667
#&gt; 
#&gt; $`iris-example`$classif.rpart
#&gt;   iter       mmce
#&gt; 1    1 0.06666667
#&gt; 2    2 0.06666667
#&gt; 3    3 0.00000000
#&gt; 4    4 0.03333333
#&gt; 5    5 0.16666667
#&gt; 
#&gt; 
#&gt; $`Sonar-example`
#&gt; $`Sonar-example`$classif.lda
#&gt;   iter      mmce
#&gt; 1    1 0.2619048
#&gt; 2    2 0.1904762
#&gt; 3    3 0.2142857
#&gt; 4    4 0.2195122
#&gt; 5    5 0.2926829
#&gt; 
#&gt; $`Sonar-example`$classif.rpart
#&gt;   iter      mmce
#&gt; 1    1 0.2857143
#&gt; 2    2 0.1904762
#&gt; 3    3 0.2142857
#&gt; 4    4 0.2926829
#&gt; 5    5 0.3658537
</code></pre>

<pre class="prettyprint well"><code class="r">## Aggregated performances
getBMRAggrPerformances(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.lda
#&gt; mmce.test.mean 
#&gt;           0.04 
#&gt; 
#&gt; $`iris-example`$classif.rpart
#&gt; mmce.test.mean 
#&gt;     0.06666667 
#&gt; 
#&gt; 
#&gt; $`Sonar-example`
#&gt; $`Sonar-example`$classif.lda
#&gt; mmce.test.mean 
#&gt;      0.2357724 
#&gt; 
#&gt; $`Sonar-example`$classif.rpart
#&gt; mmce.test.mean 
#&gt;      0.2698026
</code></pre>

<pre class="prettyprint well"><code class="r">## Predictions
pred = getBMRPredictions(res)
names(pred)
</code></pre>

<pre class="prettyprint well"><code>#&gt; [1] &quot;iris-example&quot;  &quot;Sonar-example&quot;
</code></pre>

<pre class="prettyprint well"><code class="r">head(pred$`iris-example`)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $classif.lda
#&gt; Resampled Prediction for:
#&gt; Resample description: cross-validation with 5 iterations.
#&gt; Predict: test
#&gt; Stratification: FALSE
#&gt; predict.type: response
#&gt; threshold: 
#&gt; time (mean): 0.00
#&gt;    id  truth response iter  set
#&gt; 3   3 setosa   setosa    1 test
#&gt; 7   7 setosa   setosa    1 test
#&gt; 10 10 setosa   setosa    1 test
#&gt; 13 13 setosa   setosa    1 test
#&gt; 22 22 setosa   setosa    1 test
#&gt; 36 36 setosa   setosa    1 test
#&gt; 
#&gt; $classif.rpart
#&gt; Resampled Prediction for:
#&gt; Resample description: cross-validation with 5 iterations.
#&gt; Predict: test
#&gt; Stratification: FALSE
#&gt; predict.type: response
#&gt; threshold: 
#&gt; time (mean): 0.00
#&gt;    id  truth response iter  set
#&gt; 3   3 setosa   setosa    1 test
#&gt; 7   7 setosa   setosa    1 test
#&gt; 10 10 setosa   setosa    1 test
#&gt; 13 13 setosa   setosa    1 test
#&gt; 22 22 setosa   setosa    1 test
#&gt; 36 36 setosa   setosa    1 test
</code></pre>

<p>Note that in the examples above the task and learner IDs are used to name the list elements.
The IDs can be accessed as follows:</p>
<pre class="prettyprint well"><code class="r">getBMRTaskIds(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; [1] &quot;iris-example&quot;  &quot;Sonar-example&quot;
</code></pre>

<pre class="prettyprint well"><code class="r">getBMRLearnerIds(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; [1] &quot;classif.lda&quot;   &quot;classif.rpart&quot;
#&gt; 
#&gt; $`Sonar-example`
#&gt; [1] &quot;classif.lda&quot;   &quot;classif.rpart&quot;
</code></pre>

<h2 id="nested-resampling">Nested resampling</h2>
<p>In order to get an unbiased estimate of the performance on new data,
it is generally not enough to simply use repeated cross-validation
for a given set of hyperparameters and methods (see the section about <a href="../tune/index.html">tuning</a>),
as this might produce an overly optimistic result.</p>
<p>A better (although more time-consuming) approach is to nest two
resampling methods. To keep it simple, let's take
cross-validations, which in this case is also called "double cross-validation".
In the so called "outer" cross-validation, the data is split repeatedly
into a (larger) training set and a (smaller) test set in the usual
way. Now, in every outer iteration the learner is tuned on the
training set by performing an "inner" cross-validation. The best
hyperparameters are selected and afterwards used for fitting the learner
to the complete "outer" training set. The resulting model is used to
on the "outer" test set. This results in much more reliable
estimates of the true performance distribution of the learner for unseen
data. These can now be used to estimate locations (e.g. of the mean
or median performance value) and compare learning methods in a fair
way.</p>
<p>We will see three examples to show different benchmark settings:</p>
<ul>
<li>One data set + one classification algorithm + tuning</li>
<li>Three data sets + two classification algorithms + tuning</li>
<li>One data set + two classification algorithms + feature selection</li>
</ul>
<h3 id="example-1-one-task-one-learner-tuning">Example 1: One task, one learner, tuning</h3>
<p>Now we have a <a href="http://www.rdocumentation.org/packages/mlr/functions/makeLearner.html">Learner</a> with hyperparameters and we want to find the best ones. In
that case we have two resampling levels.</p>
<p>We show an example with outer bootstrap and inner cross-validation,
our <a href="http://www.rdocumentation.org/packages/mlr/functions/makeLearner.html">Learner</a> will be <a href="http://www.rdocumentation.org/packages/kknn/functions/kknn.html">k-nearest neighbour</a>.</p>
<pre class="prettyprint well"><code class="r">## Range of hyperparameter k
ps = makeParamSet(makeDiscreteParam(&quot;k&quot;, 1:5))
ctrl = makeTuneControlGrid()

## Define &quot;inner&quot; cross-validation indices
in.rdesc = makeResampleDesc(&quot;CV&quot;, iters = 3)

## Tune k-nearest neighbor
lrn = makeTuneWrapper(&quot;classif.kknn&quot;, resampling = in.rdesc, par.set = ps, control = ctrl,
  show.info = FALSE)

## Define &quot;outer&quot; bootstrap indices
out.rdesc = makeResampleDesc(&quot;Bootstrap&quot;, iters = 5)

## Merge it into a benchmark experiment
## Choose accuracy instead of default measure mean misclassification error
res = benchmark(lrn, iris.task, out.rdesc, measure = acc, show.info = FALSE)
res
</code></pre>

<pre class="prettyprint well"><code>#&gt;        task.id         learner.id acc.test.mean
#&gt; 1 iris-example classif.kknn.tuned     0.9156964
</code></pre>

<pre class="prettyprint well"><code class="r">## What parameter setting achieved this performances?
getBMRTuneResults(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.kknn.tuned
#&gt; $`iris-example`$classif.kknn.tuned[[1]]
#&gt; Tune result:
#&gt; Op. pars: k=3
#&gt; mmce.test.mean=0.0267
#&gt; 
#&gt; $`iris-example`$classif.kknn.tuned[[2]]
#&gt; Tune result:
#&gt; Op. pars: k=4
#&gt; mmce.test.mean=0.0467
#&gt; 
#&gt; $`iris-example`$classif.kknn.tuned[[3]]
#&gt; Tune result:
#&gt; Op. pars: k=2
#&gt; mmce.test.mean=0.04
#&gt; 
#&gt; $`iris-example`$classif.kknn.tuned[[4]]
#&gt; Tune result:
#&gt; Op. pars: k=3
#&gt; mmce.test.mean=0.0467
#&gt; 
#&gt; $`iris-example`$classif.kknn.tuned[[5]]
#&gt; Tune result:
#&gt; Op. pars: k=1
#&gt; mmce.test.mean=0.02
</code></pre>

<pre class="prettyprint well"><code class="r">## What performances did we get in the single runs?
getBMRPerformances(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.kknn.tuned
#&gt;   iter       acc
#&gt; 1    1 0.9074074
#&gt; 2    2 0.9629630
#&gt; 3    3 0.9000000
#&gt; 4    4 0.9152542
#&gt; 5    5 0.8928571
</code></pre>

<p>Of course everything works the same way if we change the resampling
strategy either in the outer or inner run. They can be mixed as desired.</p>
<h3 id="example-2-two-tasks-three-learners-tuning">Example 2: Two tasks, three learners, tuning</h3>
<p>Now we look at an extensive example that shows a benchmark experiment with two data
sets, three <a href="http://www.rdocumentation.org/packages/mlr/functions/makeLearner.html">Learners</a> and <a href="../tune/index.html">tuning</a>.</p>
<pre class="prettyprint well"><code class="r">## List of learning tasks
tasks = list(iris.task, sonar.task)

## Very small grid for SVM hyperparameters
ps = makeParamSet(makeDiscreteParam(&quot;C&quot;, 2^seq(-1,1)),
  makeDiscreteParam(&quot;sigma&quot;, 2^seq(-1,1)))
ctrl = makeTuneControlGrid()

## Define &quot;inner&quot; cross-validation indices
in.rdesc = makeResampleDesc(&quot;CV&quot;, iters = 3)

## Tune a SVM
lrn = makeTuneWrapper(&quot;classif.ksvm&quot;, resampling = in.rdesc, par.set = ps, control = ctrl,
  show.info = FALSE)

## Three learners to be compared
lrns = list(makeLearner(&quot;classif.lda&quot;), makeLearner(&quot;classif.rpart&quot;), lrn)

## Define &quot;outer&quot; cross-validation indices
out.rdesc = makeResampleDesc(&quot;CV&quot;, iters = 5)

## Merge it to a benchmark experiment
res = benchmark(lrns, tasks, out.rdesc, show.info = FALSE)
res
</code></pre>

<pre class="prettyprint well"><code>#&gt;         task.id         learner.id mmce.test.mean
#&gt; 1  iris-example        classif.lda     0.02000000
#&gt; 2  iris-example      classif.rpart     0.07333333
#&gt; 3  iris-example classif.ksvm.tuned     0.04000000
#&gt; 4 Sonar-example        classif.lda     0.25493612
#&gt; 5 Sonar-example      classif.rpart     0.33112660
#&gt; 6 Sonar-example classif.ksvm.tuned     0.50917538
</code></pre>

<pre class="prettyprint well"><code class="r">## Performance values in individual runs
getBMRPerformances(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.lda
#&gt;   iter       mmce
#&gt; 1    1 0.00000000
#&gt; 2    2 0.03333333
#&gt; 3    3 0.06666667
#&gt; 4    4 0.00000000
#&gt; 5    5 0.00000000
#&gt; 
#&gt; $`iris-example`$classif.rpart
#&gt;   iter       mmce
#&gt; 1    1 0.13333333
#&gt; 2    2 0.06666667
#&gt; 3    3 0.06666667
#&gt; 4    4 0.06666667
#&gt; 5    5 0.03333333
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned
#&gt;   iter       mmce
#&gt; 1    1 0.03333333
#&gt; 2    2 0.06666667
#&gt; 3    3 0.03333333
#&gt; 4    4 0.06666667
#&gt; 5    5 0.00000000
#&gt; 
#&gt; 
#&gt; $`Sonar-example`
#&gt; $`Sonar-example`$classif.lda
#&gt;   iter      mmce
#&gt; 1    1 0.2380952
#&gt; 2    2 0.2439024
#&gt; 3    3 0.2857143
#&gt; 4    4 0.2926829
#&gt; 5    5 0.2142857
#&gt; 
#&gt; $`Sonar-example`$classif.rpart
#&gt;   iter      mmce
#&gt; 1    1 0.3809524
#&gt; 2    2 0.2439024
#&gt; 3    3 0.4523810
#&gt; 4    4 0.2926829
#&gt; 5    5 0.2857143
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned
#&gt;   iter      mmce
#&gt; 1    1 0.4285714
#&gt; 2    2 0.3414634
#&gt; 3    3 0.6428571
#&gt; 4    4 0.5853659
#&gt; 5    5 0.5476190
</code></pre>

<pre class="prettyprint well"><code class="r">## Tuned parameter for SVM
getBMRTuneResults(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.lda
#&gt; NULL
#&gt; 
#&gt; $`iris-example`$classif.rpart
#&gt; NULL
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned
#&gt; $`iris-example`$classif.ksvm.tuned[[1]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean=0.0417
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned[[2]]
#&gt; Tune result:
#&gt; Op. pars: C=1; sigma=0.5
#&gt; mmce.test.mean=0.0417
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned[[3]]
#&gt; Tune result:
#&gt; Op. pars: C=0.5; sigma=0.5
#&gt; mmce.test.mean=0.0417
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned[[4]]
#&gt; Tune result:
#&gt; Op. pars: C=0.5; sigma=0.5
#&gt; mmce.test.mean=0.0417
#&gt; 
#&gt; $`iris-example`$classif.ksvm.tuned[[5]]
#&gt; Tune result:
#&gt; Op. pars: C=1; sigma=1
#&gt; mmce.test.mean=0.0333
#&gt; 
#&gt; 
#&gt; 
#&gt; $`Sonar-example`
#&gt; $`Sonar-example`$classif.lda
#&gt; NULL
#&gt; 
#&gt; $`Sonar-example`$classif.rpart
#&gt; NULL
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned
#&gt; $`Sonar-example`$classif.ksvm.tuned[[1]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean= 0.5
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned[[2]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean=0.509
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned[[3]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean=0.428
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned[[4]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean=0.425
#&gt; 
#&gt; $`Sonar-example`$classif.ksvm.tuned[[5]]
#&gt; Tune result:
#&gt; Op. pars: C=2; sigma=0.5
#&gt; mmce.test.mean=0.567
</code></pre>

<h3 id="example-3-one-task-two-learners-feature-selection">Example 3: One task, two learners, feature selection</h3>
<p>Let's see how we can do <a href="../feature_selection/index.html">feature selection</a> in
a benchmark experiment:</p>
<pre class="prettyprint well"><code class="r">## Control object for feature selection
ctrl = makeFeatSelControlSequential(beta = 100, method = &quot;sfs&quot;)

## Inner resampling
in.rdesc = makeResampleDesc(&quot;CV&quot;, iter = 2)

## Feature selection with sequential forward search (sfs)
lrn = makeFeatSelWrapper(&quot;classif.lda&quot;, resampling = in.rdesc, control = ctrl, 
  show.info = FALSE)

## Let's compare two learners
lrns = list(makeLearner(&quot;classif.rpart&quot;), lrn)

## Define outer resampling
out.rdesc = makeResampleDesc(&quot;Subsample&quot;, iter = 3)

## Benchmark experiment
res = benchmark(tasks = iris.task, learners = lrns, resampling = out.rdesc, show.info = FALSE)
res
</code></pre>

<pre class="prettyprint well"><code>#&gt;        task.id          learner.id mmce.test.mean
#&gt; 1 iris-example       classif.rpart     0.06666667
#&gt; 2 iris-example classif.lda.featsel     0.04666667
</code></pre>

<pre class="prettyprint well"><code class="r">## Which features have been selected (in the outer resampling steps)?
getBMRFeatSelResults(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.rpart
#&gt; NULL
#&gt; 
#&gt; $`iris-example`$classif.lda.featsel
#&gt; $`iris-example`$classif.lda.featsel[[1]]
#&gt; FeatSel result:
#&gt; Features (1): Petal.Width
#&gt; mmce.test.mean=0.03
#&gt; 
#&gt; $`iris-example`$classif.lda.featsel[[2]]
#&gt; FeatSel result:
#&gt; Features (2): Sepal.Length, Petal.Width
#&gt; mmce.test.mean=0.03
#&gt; 
#&gt; $`iris-example`$classif.lda.featsel[[3]]
#&gt; FeatSel result:
#&gt; Features (2): Sepal.Length, Petal.Width
#&gt; mmce.test.mean=0.04
</code></pre>

<pre class="prettyprint well"><code class="r">## Performances on individual test data sets
getBMRPerformances(res)
</code></pre>

<pre class="prettyprint well"><code>#&gt; $`iris-example`
#&gt; $`iris-example`$classif.rpart
#&gt;   iter mmce
#&gt; 1    1 0.06
#&gt; 2    2 0.04
#&gt; 3    3 0.10
#&gt; 
#&gt; $`iris-example`$classif.lda.featsel
#&gt;   iter mmce
#&gt; 1    1 0.06
#&gt; 2    2 0.04
#&gt; 3    3 0.04
</code></pre>

</div>
        </div>

        <footer class="col-md-12">
            <hr>
            
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>

        

        <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/prettify-1.0.min.js"></script>
        <script src="../js/base.js"></script>
    </body>
</html>