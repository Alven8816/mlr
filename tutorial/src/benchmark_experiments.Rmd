# Benchmark Experiments

compare the performance of several learners on multiple data sets

function [&benchmark]


### Example 1: Two tasks and two learners

```{r eval=FALSE}
## Two learners to be compared
lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"))

## Two classification tasks
tasks = list(iris.task, bc.task)

## Define cross-validation indices
rdesc = makeResampleDesc("CV", iters = 5)

## Benchmark experiment
res = benchmark(lrns, tasks, rdesc)
res
```

The above code should be self-explanatory. In the result every row corresponds to one [&Task]
set with one [Learner](&makeLearner).
In the printed table tasks and learners are referred to by their ids.


The entries show the cross-validated mean misclassification error.

[&BenchmarkResult]
The benchmark results contain much more information, which you can
access if you want to see details. Let's have a look at the benchmark
result from the example above:

```{r eval=FALSE}
## Individual performances of the 5 cross-validation runs
getPerformances(res)

## Aggregated performances
getAggrPerformances(res)

## Predictions
pred = getPredictions(res)
head(pred$`iris-example`)
```
Note that the task ids are used to name the elements of ``pred``.



## Nested resampling

In order to get an unbiased estimate of the performance on new data,
it is generally not enough to simply use repeated cross-validation
for a given set of hyperparameters and methods (see section [tuning](tune.md)),
as this might produce an overly optimistic result.

A better (although more time-consuming) approach is to nest two
resampling methods. To keep it simple, let's take
cross-validations, which in this case is also called "double cross-validation".
In the so called "outer" cross-validation the data is split repeatedly
into a (larger) training set and a (smaller) test set in the usual
way. Now, in every outer iteration the learner is tuned on the
training set by performing an "inner" cross-validation. The best
hyperparameters are selected and afterwards used for fitting the learner
to the complete "outer" training set. The resulting model is used to
on the (outer) test set. This results in much more reliable
estimates of the true performance distribution of the learner for unseen
data. These can now be used to estimate locations (e.g. of the mean
or median performance value) and compare learning methods in a fair
way.

In the following we will see three examples to show different benchmark settings:

* One data set + one classification algorithm + tuning
* Three data sets + two classification algorithms + tuning
* One data set + two classification algorithms + variable selection


## Example 2: One task, one learner, tuning

Now we have a [Learner](&makeLearner) with hyperparameters and we want to find the best ones. In
that case we have two resampling levels.

We show an example with outer bootstrap and inner cross-validation,
our [Learner](&makeLearner) will be k-nearest neighbour.

```{r eval=FALSE}
## Range of hyperparameter k
ps = makeParamSet(makeDiscreteParam("k", 1:5))
ctrl = makeTuneControlGrid()

## Define "inner" cross-validation indices
inner.rdesc = makeResampleDesc("CV", iters = 3)

## Tune k-nearest neighbor
lrn = makeTuneWrapper("classif.kknn", resampling = inner.rdesc, par.set = ps, control = ctrl)

## Define "outer" bootstrap indices
rdesc = makeResampleDesc("Bootstrap", iters = 5)

## Merge it into a benchmark experiment
## Choose accuracy instead of default measure mean misclassification error
res = benchmark(lrn, iris.task, rdesc, measure = acc)

## What parameter setting achieved this performances?
getTuneResult(res)

## What performances did we get in the single runs?
getPerformances(res)
```

Of course everything works the same way if we change the resampling
strategy either in the outer or inner run. They can be mixed as desired.

## Example 3: Two tasks, three learners, tuning

Now we look at an extensive example that shows a benchmark experiment with two data
sets, three [Learners](&makeLearner) and [tuning](&tune.md).

```{r eval=FALSE}
## List of learning tasks
tasks = list(iris.task, bc.task)

## Very small grid for SVM hyperparameters
ps = makeParamSet(makeDiscreteParam("C", 2^seq(-1,1)),
    makeDiscreteParam("sigma", 2^seq(-1,1)))
ctrl = makeTuneControlGrid()

## Define "inner" cross-validation indices
inner.rdesc = makeResampleDesc("CV", iters = 3)

## Tune a SVM
lrn = makeTuneWrapper("classif.ksvm", resampling = inner.rdesc, par.set = ps, control = ctrl)

## Three learners to be compared
lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"), lrn)

## Define "outer" cross-validation indices
rdesc = makeResampleDesc("CV", iters = 5)

## Merge it to a benchmark experiment
res = benchmark(lrns, tasks, rdesc)

## Only for one task
getPerformances(res)

## Tuned parameter for SVM
getTuneResult(res)

## Optimal performance of the inner (!) resampling, i.e., here 3-fold cross-validation
#res["opt.perf", learner = "classif.ksvm"]
```

## Example 4: One task, two learners, variable selection

Let's see how we can do [feature selection](feature_selection.md) in
a benchmark experiment:

```{r eval=FALSE}
## Control object for feature selection
ctrl = makeFeatSelControlSequential(beta = 100, method = "sfs")

## Inner resampling
inner.rdesc = makeResampleDesc("CV", iter = 2)

## Feature selection with sequential forward search (sfs)
lrn = makeFeatSelWrapper("classif.lda", resampling = inner.rdesc, control = ctrl)

## Let's compare two learners
lrns = list(makeLearner("classif.rpart"), lrn)

## Define outer resampling
rdesc = makeResampleDesc("Subsample", iter = 3)

## Benchmark experiment
res = benchmark(tasks = iris.task, learners = lrns, resampling = rdesc)

## Which features have been selected (in the outer resampling steps)?
getFeatSelResult(res)

## Performances on individual test data sets
getPerformances(res)
```
