```{r include=FALSE}
# Not strictly necessary, but otherwise we might get NAs later on
## if 'rpart' is not installed.
library("rpart")
```

# Resampling

In order to assess the performance of a learning algorithm, resampling
strategies are usually used. 
The entire data set is split into (multiple) training and test sets.
You train a learner on each training set, predict on the corresponding test set (sometimes
on the training set as well) and calculate some performance measure.
Then the individual performance values are aggregated, typically by calculating the mean.
There exist various different resampling strategies, for example
cross-validation and bootstrap, to mention just two popular approaches.

![Resampling Figure](../../../images/resampling.png "Resampling Figure")

In **mlr** the resampling strategy can be chosen via the function [&makeResampleDesc].
The supported resampling strategies are:

* Cross-validation (``"CV"``),
* Leave-one-out cross-validation (``"LOO""``),
* Repeated cross-validation (``"RepCV"``),
* Out-of-bag bootstrap (``"Bootstrap"``),
* Subsampling (``"Subsample"``),
* Holdout (training/test) (``"Holdout"``).

The [&resample] function then evaluates the performance of a [Learner](&makeLEarner) using
the specified resampling strategy for a given machine learning task.

In the following example the performance of the
[Cox proportional hazards model](&survival::coxph) on the 
[lung](&survival::lung) data set is calculated using *3-fold cross-validation*.
Generally, in *K-fold cross-validation* the data set *D* is partitioned into *K* subsets of
(approximately) equal size.
In the *i*-th step of the *K* iterations, the *i*-th subset is
used as for testing, while the union of the remaining parts forms the training
set.
The default performance measure in survival analysis is the concordance index ([&cindex]).

```{r}
## Specify the resampling strategy (3-fold cross-validation)
rdesc = makeResampleDesc("CV", iters = 3)

## Calculate the performance
r = resample("surv.coxph", lung.task, rdesc)
r
```

``r$measures.test`` gives the value of the performance measure on the 3 individual test
data sets.
``r$aggr`` shows the aggregated performance value.
Its name ``"cindex.test.mean"`` indicates the performance measure, [&cindex], and the method used
to aggregate the 3 individual performances.
[&test.mean] is the default method and, as the name implies, takes the mean over the
performances on the 3 test data sets.
Since for cross-validation no predictions on the training data sets are necessary
``r$measures.train`` contains ``NA``s.

If predictions for the training set are required, too, ``predict = "train"``or ``predict = "both"``
in [&makeResampleDesc]
FIXME b632, b632+ bootstrap examples later.

``r$pred`` is an object of class [&ResamplePrediction].
Just as a [&Prediction] object (see the section about [making predictions](&predict.md))
``r$pred`` has an element called ``"data"`` which is a ``data.frame`` that contains the
predictions and in case of a supervised learning problem the true values of the target
variable.

```{r}
head(r$pred$data)
```

The columns ``iter`` and ``set``indicate the iteration of the resampling strategy and
if the prediction was made on the test or the training data set.


In the above example the peformance measure is the concordance index ([&cindex]).
Of course, it is  possible to compute multiple performance measures at once by 
passing a list of measures
(see also the previous section about [evaluating learner performance](performance.md)).

In the following we estimate the Dunn index ([&dunn]), the Davies-Bouldin cluster
separation measure ([&db]), and the time for training the learner ([&timetrain])
by *subsampling* with 5 iterations.
In each iteration the data set *D* is randomly partitioned into a
training and a test set according to a given percentage, e.g., 2/3
training and 1/3 test set. If there is just one iteration, the strategy
is commonly called *holdout* or *test sample estimation*.

```{r}
## Subsampling with 5 iterations and default split 2/3
rdesc = makeResampleDesc("Subsample", iters = 5)
## Subsampling with 5 iterations and 4/5 splits
rdesc = makeResampleDesc("Subsample", iters = 5, split = 4/5)

## Calculate the three performance measures
r = resample("cluster.XMeans", mtcars.task, rdesc, measures = list(dunn, db, timetrain))
r
```


## Accessing individual learner models

In each resampling iteration a [Learner](&makeLearner) is fitted on the respective training set.
By default, the resulting [&WrappedModel]s are not returned by [&resample].
If you want to keep them set ``models = TRUE``.

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

r = resample("classif.lda", iris.task, rdesc, models = TRUE)
r$models
```

Keeping only certain information instead of entire [models](&WrappedModel), for example the
variable importance in a regression tree, can be achieved using the ``extract`` argument.

```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)

## Extract the variable importance in a regression tree
r = resample("regr.rpart", bh.task, rdesc,
    extract = function(x) x$learner.model$variable.importance)
r$extract
```

The function passed to ``extract`` is applied to each [model](&WrappedModel) fitted on one of
the 3 training sets.


## Stratified resampling

For classification ``stratify` ... FIXME
```{r}
## 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3, stratify = TRUE)

r = resample("classif.lda", iris.task, rdesc, models = TRUE)
r$models
```

## Resample descriptions and resample instances

--
Resampling strategies concern the process of sampling new data sets that are
subsets of the main data set *D*. One wants to generate various training sets
that the learning method can fit models to and test sets that those models can
be evaluated on. We assume that every resampling strategy consists of a
number of iterations and each one defines a set of indices into *D* that determine
the respective training and test sets.
--


As shown above the function [&makeResampleDesc] is used to specify the resampling strategy.

```{r}
rdesc = makeResampleDesc("CV", iters = 3)
str(rdesc)
```

The result ``rdesc``is an object of class [ResampleDesc](&makeResampleDesc) and contains,
as the name implies, a description of the resampling strategy.
Basically, this is an instruction for randomly drawing training and test sets including
the necessary parameters like the number of iterations, the sizes of the training and test
sets etc.

Based on this description, the data set is randomly separated into multiple training and
test sets.
The resulting set of index vectors indicating the training and test examples for each iteration
is stored in a [ResampleInstance](&makeResampleInstance).

If a [&ResampleDesc] is passed to [&resample] it is instantiated internally.
It is also possible to pass a [&ResampleInstance] to [&resample].

A resample instance can be created by function [&makeResampleInstance] given a resample
description and either the size of the data set at hand or the [&Task].
It basically takes the description object and performs the random
drawing of indices to separate the data into training and test.

```{r}
## Create a resample instance based an a task
rin = makeResampleInstance(rdesc, task = iris.task)
rin

## Create a resample instance given the size of a data set
rin = makeResampleInstance(rdesc, size = nrow(iris))
str(rin)

## Access the indices of the training observations in iteration 3
rin$train.inds[[3]]
```

While having all those separate objects, resample descriptions, instances and the resample
function seems overly complicated, it has several advantages:

* Having resample instances allows for paired experiments, that is comparing the performance
  of several learners on exactly the same training and test sets.
  This is particularly useful if you want to add another method to a comparison experiment
  you already did.
  ```{r}
  rdesc = makeResampleDesc("CV", iters = 3)
  rin = makeResampleInstance(rdesc, task = iris.task)
  
  ## Calculate the performance of two learners based on the same resample instance
  r.lda = resample("classif.lda", iris.task, rin)
  r.rpart = resample("classif.rpart", iris.task, rin)
  ```
* It is easy to add other resampling methods later on. You can
  simply derive from the [ResampleInstance](&makeResampleInstance)
  class, but you do not have to touch any methods that use the
  resampling strategy.

FIXME: Sometimes one only wants to describe the drawing process, while in other instances one wants
to create the specific index sets.

As mentioned above, when calling [&makeResampleInstance] the index sets are drawn randomly.
Mainly, for *holdout* (test/train) estimation you may want to specify the training and test
sets manually.
This can be done using function [&makeFixedHoldoutInstance].

```{r}
rin = makeFixedHoldoutInstance(train.inds = 1:100, test.inds = 101:150, size = 150)
rin
```


## Aggregation

performance values resulting from individual iterations have to be aggregated.
default mean

Every single resampling
iteration is handled as described in the explanation above, i.e. you
train a model on the training part of the data set, predict on the test set
and compare predicted and true labels to compute some performance
measure. This is done in every iteration so that you have
e.g. ten performance values in the case of 10-fold cross-validation.
The question arises of how to aggregate those values. You can specify
that explicitly, the default is to use the mean.

For example, a 10-fold cross validation computes 10 values for the chosen
performance measure.
The aggregated value is the mean of these 10 numbers.
**mlr** knows how to handle it because each [&Measure] knows how it is aggregated:

```{r}
## Mean misclassification error
mmce$aggr

## Root mean square error
rmse$aggr 
```

change via function [&setAggregation]
[&aggregations] for available methods

You can also create a new [&Measure] by using a different aggregation (see [&aggregations]).
[&setAggregation]


### Example: Calculating mean and standard deviation of the error rate
```{r}
mmce.test.sd = setAggregation(mmce, test.sd)
r = resample("classif.rpart", iris.task, iris.rin, measures = list(mmce, mmce.test.sd))
r$aggr
```


### Example: Calculating the training error


### Example: Bootstrap

*B* new data sets *D_1* to *D_B* are drawn from
*D* with replacement, each of the same size as *D*.
In the *i*-th iteration *D_i* forms the training set,
while the remaining elements from *D*, i.e., elements not
in the training set, form the test set.

<!--(
                     |resampling_desc_figure|

                     |resampling_nested_resampling_figure|
)-->

b632 linear combination of the training error and the bootstrap error
b632+

predictions on the training set
aggregation strategy


```{r}
rdesc = makeResampleDesc("Bootstrap", predict = "both", iters = 10)
b632.mmce = setAggregation(mmce, b632)
b632plus.mmce = setAggregation(mmce, b632plus)
b632.mmce

r = resample("classif.rpart", iris.task, rdesc,
    measures = list(mmce, b632.mmce, b632plus.mmce), show.info = FALSE)
head(r$measures.train)
r$aggr
```


## Convenience functions

**mlr** provides some convenience functions for the frequently used resampling strategies,
for example [&holdout], [&crossval] or [bootstrap](&bootstrapB632).
```{r}
r = holdout("regr.lm", bh.task, measures = list(mse, msa))
r = crossval("classif.lda", iris.task, iters = 3, measures = list(mmce, ber))
r = bootstrapB632()
```

recommended way is resample, 
maximal control, resample descs and instances


## Pre-defined resample instances

For each of the pre-defined [&Task]s **mlr** provides pre-defined resample instances for
3-fold cross-validation. 
These instances are occasionally used in this tutorial.

```{r echo=FALSE, results="asis"}
d = data(package = "mlr")
d = d$results
df = as.data.frame(d[seq(1,nrow(d),2), c("Item", "Title")])
names(df)[1] = "Task"
pandoc.table(df, style = "rmarkdown", split.tables = Inf, split.cells = Inf,
	justify = c("left", "left"))
```
