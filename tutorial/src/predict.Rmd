# Predicting Outcomes for New Data

Predicting the target values for new observations is
implemented the same way as most of the other predict methods in **R**.
In general, all you need to do is call [predict](&predict.WrappedModel) on the object
returned by [&train] and pass the data to be predicted.

There are two possibilities to pass the data.

* Either pass the [&Task] via the ``task`` argument
* or pass a ``data.frame`` via the ``newdata`` argument.

The first way is preferable if you want predictions for data already included in the [&Task].
Just as [&train], the [predict](&predict.WrappedModel) function has a ``subset`` argument,
so you can set aside different portions of the data in [&Task] for training and prediction.
In the following example we fit a [gradient boosting machine](&gbm::gbm) to every second
observation of the [BostonHousing](&mlbench::BostonHousing) data set and make predictions
on the remaining data.

```{r}
n = bh.task$task.desc$size
train.set = seq(1, n, 2)
test.set = seq(2, n, 2)
lrn = makeLearner("regr.gbm", n.trees = 100)
mod = train(lrn, bh.task, subset = train.set)

task.pred = predict(mod, task = bh.task, subset = test.set)
task.pred
```

The second way is useful if you want to predict data not included in the [&Task].
In the following we cluster the ``iris`` data set without the target variable.
All observations with an odd index are included in the [&Task] and used for training.
Predictions are made for the remaining observations.

```{r}
n = nrow(iris)
train.set = seq(1, n, 2)
test.set = seq(2, n, 2)
task = makeClusterTask(data = iris[train.set,-5])
mod = train("cluster.XMeans", task)

newdata.pred = predict(mod, newdata = iris[,-5], subset = test.set)
newdata.pred
```

The result of [predict](&predict.WrappedModel) naturally depends on the nature of the [&Task]
and the type of prediction chosen when creating the [Learner](&makeLearner).
For example in case of survival analysis the default is to predict the response.
For the [Cox proportional hazards model](&Surv::coxph) we get the values of the linear
predictor as shown in the following.

```{r}
n = lung.task$task.desc$size
train.set = seq(1, n, 2)
test.set = seq(2, n, 2)
mod = train("surv.coxph", lung.task, subset = train.set)

pred = predict(mod, task = lung.task, subset = test.set)
pred
```

It is also possible to predict time-dependent probabilities.
In order to do so you have to create a [Learner](&makeLearner) and set ``predict.type = "prob"``.

```{r}
lrn = makeLearner("surv.coxph", predict.type = "prob")
mod = train(lrn, lung.task, subset = train.set)

pred = predict(mod, task = lung.task, subset = test.set)
head(pred$data[,1:8])
```

Predictions are encapsulated in a special [&Prediction] object.


## Accessing the prediction

A [&Prediction] object is a ``list``. The most important element is ``"data"`` which is a
``data.frame`` that contains columns with the true values of the target variable (in case of
supervised learning problems) and the predictions.

In the following the predictions on the [BostonHousing](&mlbench::BostonHousing) and the
[iris](&datasets::iris) data sets are shown.

```{r}
head(task.pred$data)

head(newdata.pred$data)
```

As you can see when predicting from a [&Task], the resulting ``data.frame`` contains an
additional column, called ``id``, which tells us which element in the original data set
the prediction corresponds to.

In case of classification problems there are some more functions to access the [&Prediction].


### Classification

In case of classification, per default, class labels are predicted.

```{r}
## Linear discriminant analysis on the iris data set
mod = train("classif.lda", task = iris.task)

pred = predict(mod, task = iris.task)
pred
```

A confusion matrix can be obtained by calling [&getConfMatrix].
```{r}
getConfMatrix(pred)
```

In order to get predicted posterior probabilities we have to create a [Learner](&makeLearner)
with the appropriate ``predict.type``.

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)

pred = predict(mod, newdata = iris)
head(pred$data)
```

As you can see, in addition to the probabilities, class labels are predicted
by choosing the class with the maximum probability and breaking ties at random.

The predicted posterior probabilities can be accessed via the [&getProbabilities] function.
```{r}
head(getProbabilities(pred))
```


### Binary classification

In case of binary classification, two things are worth mentioning. As you may recall,
we can specify which of the two classes should be considered as positive class when
generating the [&Task]. Moreover, we can set the threshold value that is used 
to map the predicted posterior probabilities to class labels.
Note that for this purpose we need to create a [Learner](&makeLearner) that predicts
probabilities.

To illustrate binary classification, we use the [BreastCancer](&mlbench::BreastCancer)
data set from the
[mlbench](http://cran.r-project.org/web/packages/mlbench/index.html) package.

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = bc.task)

pred = predict(mod, task = bc.task)
pred
pred$threshold
```

As you can see the default threshold is 0.5, that is an example is assigned to the class
with maximum posterior probability.

We can adjust the threshold used using the function [&setThreshold].
Now, we set the threshold for the *positive* class to 0.8 (that is, an example is assigned
to the positive class if its posterior probability exceeds 0.8):

```{r}
pred = setThreshold(pred, 0.8)
pred
pred$threshold
```

Note that in the binary case [&getProbabilities] extracts the posterior probabilities
of the positive class only.

```{r}
head(getProbabilities(pred))
```


## Visualizing the prediction

Function [&plotLearnerPrediction] allows to visualize predictions, e.g., for teaching purposes
or exploring models.
It trains the chosen learning method for 1 or 2 selected features and then displays the
predictions via [ggplot](ggplot2::ggplot). 

For *classification*, we get a scatterplot of 2 features (per default the first 2 in the data set).
The plotting symbols show the true class labels of the data points.
The color indicates misclassified observations.
The posterior probabilities (if the learner under consideration supports this)
are represented by the background color.

The plot title displays the ID of the learner (in the following example CART),
its parameters, its training performance and its cross-validation performance.
[&mmce] stands for *mean misclassification error*, i.e., the error rate.
See the sections about [assessing the performance of a learner](&performance.md) and
[resampling](&resample.md) for further explanations.

```{r}
lrn = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)
```

For *regression*, there exist two types of plots.
The 1D plot shows the target values in dependence of 1 feature, the regression curve and
potentially (if the chosen learner supports this) the estimated standard error.

```{r}
plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)
```

The 2D variant, as in the classification case, generates a scatterplot of 2 features.
The estimated mean is shown via the background color.
The plot does not represent the estimated standard error.

```{r}
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
```
