# ROC Analysis

The receiver operating characteristic (ROC) curve is a graphical plot
that illustrates the performance of a binary classifier.
It plots the false positive rate (fall-out) on the horizontal axis versus
the true positive rate (sensitivity) on the vertical axis for various settings of
the threshold that maps predicted posterior probabilities to class labels.

A ROC curve is useful if one is unsure about the threshold setting, for example if the
classifications costs (that would allow to calculate an optimal threshold value) are unknown. 

The AUC (area under curve) summarizes the performance of the learner over all possible threshold
values.
**mlr** provides the [&auc] performance measure for binary classification and also a
generalization of the AUC for the multi-class case ([multiclass.auc](&pROC::muticlass.auc))
from package [pROC](http://cran.r-project.org/web/packages/pROC/index.html).


## Plotting the ROC curve

The **mlr** function [&asROCRPrediction] converts predictions (resulting from
calling [predict](predict.WrapperModel), [&resample], or [&benchmark]) to a format that
[ROCR](http://cran.r-project.org/web/packages/ROCR/index.html) can handle.
Then functions in [ROCR](http://cran.r-project.org/web/packages/ROCR/index.html) are used
to plot the ROC curve.

In the following example we show how to compare the performance of two learners.

### Comparing two learners

Note that the [learners](learner.md) have to be capable of predicting probabilities.
Have a look at the [table of learners](integrated_learners.md)
or run `listLearners(prob = TRUE)` to get a list of all learners that support this.
We use the [BreastCancer](&mlbench::BreastCancer) data set from package
[mlbench](http://cran.r-project.org/web/packages/mlbench/index.html).

```{r}
## Linear discriminant analysis and support vector machine with RBF kernel
lrn1 = makeLearner("classif.lda", predict.type = "prob")
lrn2 = makeLearner("classif.ksvm", predict.type = "prob")
```

Afterwards, we perform [resampling](resample.md) to obtain predictions for each fold.

```{r}
## Perform a 10-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 10)
rin = makeResampleInstance(rdesc, bc.task)
## Calculate the error rate (for threshold 0.5) and the AUC
ms = list(mmce, auc)
r1 = resample(lrn1, bc.task, rin, measures = ms, show.info = FALSE)
r2 = resample(lrn2, bc.task, rin, measures = ms, show.info = FALSE)
```

Now we have to convert each prediction within the resample result to a
[ROCR prediction](&ROCR::prediction-class) using the [&asROCRPrediction] function.
Afterwards we let [ROCR](http://cran.r-project.org/web/packages/ROCR/index.html)
calculate the true and false positive rate using the function [performance](&ROCR::performance)
and [plot](&ROCR::plot.performance) the ROC curve.
As we have one curve for each learner and each cross validation fold we might want to
average the curves from the cross validation by using `avg = "threshold"`.
Otherwise, the plot method will draw one curve for each fold.
For details, see [plot.performance](&ROCR::plot.performance).

```{r ROCRaverage}
pred1 = asROCRPrediction(r1$pred)
pred2 = asROCRPrediction(r2$pred)
perf1 = ROCR::performance(pred1, "tpr", "fpr")
perf2 = ROCR::performance(pred2, "tpr", "fpr")
plot(perf1, col = "blue", avg = "threshold")
plot(perf2, col = "red", avg = "threshold", add = TRUE)
legend("bottomright", legend = c("lda", "ksvm"), lty = 1, col = c("blue", "red"))
```

In order to create pooled ROC curves we can also cheat a bit by manually setting the class attribute
of the prediction object from [&ResamplePrediction] to [&Prediction].
```{r ROCRpooled}
r1p = r1$pred
r2p = r2$pred
class(r1p) = "Prediction"
class(r2p) = "Prediction"
pred1 = asROCRPrediction(r1p)
pred2 = asROCRPrediction(r2p)
perf1 = ROCR::performance(pred1, "tpr", "fpr")
perf2 = ROCR::performance(pred2, "tpr", "fpr")
plot(perf1, col = "blue")
plot(perf2, col = "red", add = TRUE)
```


## Further performance plots

Note that you can easily create other standard evaluation plots like *lift charts* by
calling [&ROCR::performance] with the appropriate performance measures.
```{r Liftaverage}
perf1 = ROCR::performance(pred1, "lift", "rpp")
perf2 = ROCR::performance(pred2, "lift", "rpp")
plot(perf1, col = "blue", avg = "threshold")
plot(perf2, col = "red", avg = "threshold", add = TRUE)
legend("topright", legend = c("lda", "ksvm"), lty = 1, col = c("blue", "red"))
```

## Tuning the threshold

Of course you can also select a threshold without the help of a ROC curve by directly tuning it the following way:
```{r}
## Area Under the Curve and Mean Missclassification Error before the tuning.
performance(r1$pred, measures = list(auc,mmce))
r1$pred$threshold
## Tuning the threshold.
(tune.res = tuneThreshold(pred = r1$pred, measure = mmce))
pred.tuned = setThreshold(r1$pred, tune.res$th)
## Performance after the tuning.
performance(pred.tuned, measures = list(auc,mmce))
```
