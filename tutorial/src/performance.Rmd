# Evaluating Learner Performance

The quality of the predictions of a model in **mlr** can be assessed w.r.t. a
number of different performance measures.
In order to calculate the performance measures, call [&performance] on the object
returned by [predict](&predict.WrappedModel) and specify the desired performance measure(s).


## Implemented performance measures

**mlr** provides a large number of performance measures for all types of learning problems.
Typical performance measures for *classification* are mean misclassification error ([&mmce]),
accuracy ([&acc]) or measures based on [ROC analysis](&roc_analysis.md).
For *regression* usually mean of squared errors ([&mse]) or mean of absolute errors ([&mae])
are considered.
For *clustering* tasks, measures such as the Dunn index ([&dunn]) are provided,
while for *survival* predictions, the Concordance Index ([&cindex]) is
supported, and for *cost-sensitive* predictions the misclassification penalty
([&mcp]) and others. It is also possible to access the time to train the
learner ([&timetrain]), the time to compute the prediction ([&timepredict]) and their 
sum ([&timeboth]) as performance measures.

To see which performance measures are implemented, have a look at the
[table of performance measures](measures.md) and the [&measures] documentation page.

If you want to implement an additional measure or include a measure with
non-standard misclassification costs, go to section
[Create Custom Measures](&create_measure.md).


## Calculate performance measures

In the following example we fit a [gradient boosting machine](&gbm::gbm) on a subset of the
[BostonHousing](&mlbench::BostonHousing) data set and calculate the mean squared error
([&mse]) on the remaining observations.

```{r}
n = bh.task$task.desc$size
lrn = makeLearner("regr.gbm", n.trees = 1000)
mod = train(lrn, task = bh.task, subset = seq(1,n,2))
pred = predict(mod, task = bh.task, subset = seq(2,n,2))

performance(pred)
```

For each type of learning problem there is a default performance measure.
For regression the mean squared error ([&mse]) is calculated.

Compute the median of squared errors ([&medse]) instead.

```{r}
performance(pred, measures = medse)
```

Of course, we can also calculate multiple performance measures at once by simply passing a 
list of measures which can also include [your own measure](create_measure.md).

Calculate the mean squared error, median squared error and mean absolute error ([&mae]).

```{r}
performance(pred, measures = list(mse, medse, mae))
```

For the other types of learning problems and measures, calculating the performance basically 
works in the same way.


### Requirements of performance measures

Note that in order to calculate some performance measures it is required that you pass the 
[&Task] or the [fitted model](&makeWrappedModel) additionally to the [&Prediction].

For example in order to assess the time needed for training ([&timetrain]), the fitted
model has to be passed.
```{r}
performance(pred, measures = timetrain, model = mod)
```

For many performance measures in cluster analysis the [&Task] is required.
```{r}
lrn = makeLearner("cluster.SimpleKMeans", N = 3)
mod = train(lrn, mtcars.task)
pred = predict(mod, task = mtcars.task)

## Calculate the Dunn index
performance(pred, measures = dunn, task = mtcars.task)
```

Moreover, some measures require a certain type of prediction.
For example in binary classification in order to calculate the AUC ([&auc]) - the area
under the ROC (receiver operating characteristic) curve, we have to make sure that posterior
probabilities are predicted.
For more information on ROC analysis, see section [ROC Analysis](roc_analysis.md).

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)
pred = predict(mod, task = sonar.task)

performance(pred, measures = auc)
```

Also bear in mind that many of the performance measures available for classification as,
e.g., the false positive rate ([&fpr]) are only suitable for binary problems.


## Access a performance measure

Performance measures in **mlr** are objects of class [Measure](&makeMeasure).
If you are interested in the properties/requirements of a single measure you can access it directly.
See the help page of [Measure](&makeMeasure) for information on the individual slots.

```{r}
## Mean misclassification error
str(mmce)
```


## Find a suitable performance measure

The properties and requirements of the individual measures are shown in the
[table of performance measures](&measures.md).

If you would like a list of available measures with certain properties or suitable for a
certain learning [&Task] use the function [&listMeasures].

```{r}
## Performance measures for classification with multiple classes
listMeasures("classif", properties = "classif.multi")
## Performance measure suitable for the iris classification task
listMeasures(iris.task)
```


## Binary classification: Plot performance(s) versus threshold

As you may recall (see the previous section about [making predictions](&predict.md))
in binary classification we can adjust the threshold used to map probabilities to class labels.
Helpful in this regard is function [&plotThreshVsPerf] that generates a plot of the
learner performance versus the threshold.
Later on we will show how to find an optimal threshold value by [tuning](tune.md).

In the following example we consider the [BreastCancer](&mlbench::BreastCancer) data set and
plot the false positive rate ([&fpr]), the false negative rate ([&fnr]) as well as the
misclassification rate ([&mmce]) for all possible threshold values.

```{r}
lrn = makeLearner("classif.lda", predict.type = "prob")
n = bc.task$task.desc$size
mod = train(lrn, task = bc.task, subset = seq(1,n,2))
pred = predict(mod, task = bc.task, subset = seq(2,n,2))

## Performance for the default threshold 0.5
performance(pred, measures = list(fpr, fnr, mmce))
## Plot false negative and positive rates as well as the error rate versus the threshold
plotThreshVsPerf(pred, measures = list(fpr, fnr, mmce))
```
