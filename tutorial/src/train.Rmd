# Training a Learner

Training a learner means fitting a model to a given data set.
In the [%mlr] package this can be done by calling the function [&train]
with a [Learner](&makeLearner) and a suitable [&Task].

We start with a survival analysis example and train a
[Cox proportional hazards model](&survival::coxph)
on the [lung](&survival::lung) data set.

```{r}
data(lung, package = "survival")
lung$status = (lung$status == 2)
task = makeSurvTask(data = lung, target = c("time", "status"))
lrn = makeLearner("surv.coxph")

mod = train(lrn, task)
mod
```

In the above example, creating a [Learner](&makeLearner) explicitly is not strictly necessary.
As a general rule, you have to create a [Learner](&makeLearner) if you want to change any defaults
by, e.g., setting hyperparameter values or changing the type of prediction.
Otherwise, [&train] and many other functions accept a character string naming the learning
method.

```{r}
mod = train("surv.coxph", task)
mod
```

Training a learner works the same way for every type of [&Task].
Here is a cluster analysis example using the data set [mtcars](&dataset::mtcars).
We create a [Learner](&makeLearner) because we want to set one of the hyperparameter values, the number of clusters.

```{r}
task = makeClusterTask(data = mtcars)
lrn = makeLearner("cluster.SimpleKMeans", N = 3)

mod = train(lrn, task)
mod
```

Optionally, only a subset of the data, specified by an index set, can be used to
train the learner. This set is passed using the ``subset`` argument of [&train].

We fit a simple linear regression model to the [BostonHousing](&mlbench::BostonHousing) data set.
The object [&bh.task] is the regression [&Task] on the [BostonHousing](&mlbench::BostonHousing)
data set provided by [%mlr].

```{r}
## Number of observations
n = bh.task$task.desc$size
## Use 1/3 of the observations for training
train.set = sample(n, size = n/3)
## Train the learner
mod = train("regr.lm", bh.task, subset = train.set)
mod
```

Note that all standard [resampling strategies](resample.md) are supported.
Therefore you usually do not have to subset the data yourself.

Moreover, if the [Learner](&makeLearner) supports this, you can specify observation ``weights``
that reflect the relevance of examples in the training process.

For example, in the [BreastCancer](&mlbench::BreastCancer) data set class benign is almost
twice as frequent as class malignant.
If both classes should have equal importance in training the classifier we can weight the
examples according to the class frequencies in the data set as shown in the following
**R** code (see also the section about [imbalanced classification problems](over_and_undersampling.md)).

```{r}
## Calculate the observation weights
tar = getTaskTargets(bc.task)
tab = as.numeric(table(tar))
w = 1/tab[tar]

train("classif.cforest", task = bc.task, weights = w)
```

As you may recall, it is also possible to pass observation weights when creating the
[&Task].
Naturally, it makes sense to specify ``weights`` in [make<type>Task](&Task) if those weights
should always be used for the learning task and in [&train] if this is not the case since, e.g.,
some of the learners you want to use cannot deal with weights or you want to try different weights.
The weights in [&train] overwrite the weights in [&Task].


## Wrapped models

[&train] returns an object of class [WrappedModel](&makeWrappedModel), which wraps the
particular model of the underlying **R** learning method. The wrapped model contains the output as well as the actual model.
It can subsequently be used to perform a [prediction](&predict.WrappedModel) for new observations.


In order to access the underlying model we can use the function [&getLearnerModel].
In the following example we get an object of class [lm](&stats::lm).

```{r}
mod = train("regr.lm", bh.task, subset = train.set)
getLearnerModel(mod)
```
