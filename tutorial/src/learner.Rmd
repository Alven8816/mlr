# Learners
The following classes provide a unified interface to all popular machine learning methods in **R**:
(cost-sensitive) classification, regression, survival analysis, and clustering.
Many are already integrated, others are not, but the package is specifically designed to make extensions simple.

The [table of integrated learners](integrated_learners.md) shows the already
implemented machine learning methods and their properties.
If your favorite method is missing, either [open an issue](https://github.com/berndbischl/mlr/issues) or see the part of the tutorial on [how to extend the package yourself](create_learner.md).
This basic introduction demonstrates how to use already implemented learners.


## Constructing a learner
A learner in [%mlr] is generated by calling [&makeLearner].
In the constructor you can do the following things, apart from selecting the type of learner to create:

* Set hyperparameters.
* Control the output for later prediction, e.g., for classification
  whether you want a factor of predicted class labels or probabilities.
* Set an ID to name the object (some methods will later use this ID to name results or annotate plots).

```{r}
## Regression gradient boosting machine, specify hyperparameters via a list
regr.lrn = makeLearner("regr.gbm", par.vals = list(n.trees = 500, interaction.depth = 3))

## Classification tree, set it up for predicting probabilities
classif.lrn = makeLearner("classif.randomForest", predict.type = "prob", fix.factors = TRUE)

## Cox proportional hazards model with custom name
surv.lrn = makeLearner("surv.coxph", id = "cph")

## K-means with 5 clusters
cluster.lrn = makeLearner("cluster.SimpleKMeans", N = 5)
```

The first argument specifies which algorithm to use.
The naming convention is ``classif.<R_method_name>`` for
classification methods, ``regr.<R_method_name>`` for regression methods,
``surv.<R_method_name>`` for survival analysis, and ``cluster.<R_method_name>``
for clustering methods.

Hyperparameter values can be specified either via the ``...`` argument or as a [list](&base::list)
via ``par.vals``.

Occasionally, [factor](&base::factor) features may cause problems when fewer levels are present in the
test data than in the training data.
By setting ``fix.factors = TRUE`` these are avoided by adding a factor level for missing data in the test data set.

Let's have a look at two of the learners created above.

```{r}
surv.lrn

classif.lrn
```

All generated learners are objects of class [Learner](&makeLearner).
This class contains the properties of the method, e.g., which types of features it can handle,
what can be output during prediction, and whether multi-class problems,
observations weights or missing values are supported.

As you might have noticed, there is currently no special learner class for cost-sensitive classification.
There are several ways to generate cost-sensitive learners from regression and classification
learners. This is explained in greater detail in the section about
[cost-sensitive classification](cost_sensitive_classif.md).


## Accessing a learner
The [Learner](&makeLearner) object is a [list](&base::list) and the following elements contain
information regarding the hyperparameters and the type of prediction.

```{r}
## Get the configured hyperparameter settings that deviate from the defaults:
cluster.lrn$par.vals

## Get the set of hyperparameters:
classif.lrn$par.set

## Get the type of prediction:
regr.lrn$predict.type
```

Slot ``$par.set`` is an object of class [ParamSet](&ParamHelpers::makeParamSet).
It contains, among others, the type of parameters (e.g., numeric, logical), potential
default values and the range of allowed values.

Moreover, [%mlr] provides functions [&getHyperPars] and [&getParamSet] to access the
selected hyperparameter values and the parameter set.
These are particularly useful in case of a wrapped [Learner](&makeLearner),
for example if a learner is fused with a feature selection strategy, and both, the learner
as well the feature selection method, have hyperparameters.
For details see the section on [wrapped learners](wrapper.md).

```{r}
## Get current parameter settings
getHyperPars(cluster.lrn)

## Get a description of all possible parameter settings
getParamSet(classif.lrn)
```

We can also use [&getParamSet] to get a quick overview about the available hyperparameters
and defaults of a learning method without explicitly constructing it.

```{r}
getParamSet("classif.randomForest")
```


## Modifying a learner

There are also some functions that enable you to change certain aspects
of a [Learner](&makeLearner) without needing to create a new [Learner](&makeLearner) from scratch.
Here are some examples.

```{r}
## Change the ID:
surv.lrn = setId(surv.lrn, "CoxModel")
surv.lrn

## Change the prediction type, predict a factor with class labels instead of probabilities:
classif.lrn = setPredictType(classif.lrn, "response")

## Change hyperparameter values:
cluster.lrn = setHyperPars(cluster.lrn, N = 4)

## Go back to default hyperparameter values:
regr.lrn = removeHyperPars(regr.lrn, c("n.trees", "interaction.depth"))
```

## Listing learners
The list of all learners and their respective properties are shown in the [Appendix](integrated_learners.md).

If you would like a list of available learners, maybe only with certain properties or suitable for a
certain learning [&Task] use the function [&listLearners].

```{r}
## List everything in mlr
head(listLearners())
## List classifiers that can output probabilities
head(listLearners("classif", properties = "prob"))
## List classifiers that can be applied to iris (i.e. multiclass) and output probabilities
head(listLearners(iris.task, properties = "prob"))
## The calls above return character vectors, but you can also create learner objects
head(listLearners("cluster", create = TRUE), 2)
```

