#' Perform an mbo run on a test function and and visualize what happens.
#' 
#' Usually used for 1D or 2D examples, 
#' useful for figuring out how stuff works and for teaching purposes.
#' Currently only parameter spaces with numerical parameters are supported.
#' For visualization, run \code{plot} on the resulting object.
#' What is displayed is documented here: \code{\link{plot.MBOExampleRun}}.
#'
#' Please note the following things:
#' - The true objective function (and later everything which is predicted from our surrogate model) 
#'   is evaluated on a regular spaced grid. These evaluations are stored in the result object.
#'   You can control the resultion of this grid via \code{points.per.dim}.
#' - In every iteration the fitted, approximating surrogate model is stored in the result object
#'   (via \code{save.model.at} in \code{control}) so we can later visualize it quicky.
#' - To save a few lines of code, you can also pass a test function from package
#'   \code{soobench} to \code{fun}. In this case \code{par.set} and \code{global.opt}
#'   are autogenerated by default and the objective function works even though its
#'   signature does not adhere the \dQuote{parameter must be a list}-specification.
#' 
#' @param fun [\code{function}]\cr
#'   See \code{\link{mbo}}.
#' @param par.set [\code{\link[ParamHelpers]{ParamSet}}]\cr
#'   See \code{\link{mbo}}.
#' @param global.opt [\code{numeric(1)}]\cr
#'   Objective value of global optimum, if known. 
#'   \code{NA} means not known, which is the default.
#'   Is used to calculate and display the gap between the optimum
#'   and the current best value in the plot.
#'   Default is \code{NA}.
#' @param learner [\code{\link[mlr]{Learner}}]\cr
#'   See \code{\link{mbo}}.
#'   Default is mlr learner \dQuote{regr.km}, which is kriging from package
#'   DiceKriging. 
#'   \code{nugget.estim} is set to \code{TRUE} depending on whether we have 
#'   noisy observations or not.
#FIXME use other learner if we have factors
#' @param control [\code{\link{MBOControl}}]\cr
#'   See \code{\link{mbo}}.
#' @param  points.per.dim [\code{integer}]\cr
#'   Number of (regular spaced) locations at which to 
#'   sample the \code{fun} function per dimension.
#'   Default is 50.
#' @param noisy.evals [\code{integer(1)}]\cr
#'   Number of function evaluations per point if \code{fun} is noisy.
#'   Default is 10.
#' @param show.info [\code{logical(1)}]\cr
#'   Verbose output on console?
#'   Default is \code{TRUE}.
#' @return [\code{list}]:
#'   \item{xseq [\code{numeric}]}{Sequence of x values from the domain of \code{fun}.}
#'   \item{yseq [\code{numeric}]}{Sequence of evaluated points.}
#'   \item{ymat [\code{numeric}]}{Sequence of evaluated points.}
#'   \item{mbo.res [\code{\link{MBOResult}}]}{MBO result object.}
#'   \item{par.set [\code{\link[ParamHelpers]{ParamSet}}]}{See argument.}
#'   \item{learner [\code{\link[mlr]{Learner}}]}{See argument.}
#'   \item{control [\code{\link{MBOControl}}]}{See argument.}


#FIXME add ...
#FIXME doc soobench and add suggsts
#FIXME can we allow functions with simpler signature?
exampleRun = function(fun, par.set, global.opt=NA_real_, learner, control, 
  points.per.dim=50, noisy.evals=10, show.info=TRUE) {
  
  checkArg(fun, "function")
  if (missing(par.set) && inherits(fun, "soo_function")) 
    par.set = makeNumericParamSet(lower=lower_bounds(fun), upper=upper_bounds(fun))
  else
    checkArg(par.set, "ParamSet")

  if (missing(global.opt) && inherits(fun, "soo_function")) 
    global.opt = global_minimum(fun)$val
  else
    checkArg(global.opt, "numeric", len=1L, na.ok=TRUE)

  par.types = extractSubList(par.set$pars, "type")

  checkArg(control, "MBOControl")
  noisy = control$noisy
  if (missing(learner)) {
    # set random forest as default learner if discrete params occur
    if (any(par.types %in% c("discrete"))) {
      # FIXME: set further params 
      learner = makeLearner("regr.randomForest")
    } else {
      learner = makeLearner("regr.km", covtype="matern5_2", predict.type="se", nugget.estim=noisy)
    }
  } else {
    checkArg(learner, "Learner")
  }
  points.per.dim = convertInteger(points.per.dim)
  checkArg(points.per.dim, "integer", len = 1L, na.ok = FALSE, lower=1L)
  noisy.evals = convertInteger(noisy.evals)
  checkArg(noisy.evals, "integer", len = 1L, na.ok = FALSE, lower=1L)
  checkArg(show.info, "logical", len = 1L, na.ok = FALSE)
  n.params = sum(getParamLengths(par.set))
  #FIXME: what do we allow?
  #if (n.params != 1L)
  #  stopf("exampleRun can currently only be used for 1D functions, but you have: %iD.", n.params)
  #ps2 = filterParams(par.set, "numeric")
  #if (length(ps2$pars) != length(par.set$pars))
  #  stopf("exampleRun can currently only be used numeric parameters.")
  
  control$save.model.at=0:control$iters
  names.x = getParamIds(par.set, repeated=TRUE, with.nr=TRUE)
  name.y = control$y.name
  
  #FIXME maybe allow 1 discrete or int param as well!
  
  if (show.info) {
    messagef("Evaluating true objective function at %s points.",
      if(!noisy) {
        if(n.params == 1) 
        sprintf("%i", points.per.dim)
        else
          sprintf("%i x %i", points.per.dim, points.per.dim)
      }
    )
  }

  if (inherits(fun, "soo_function"))
    fun = makeMBOFunction(fun)
  
  lower = getLower(par.set)
  upper = getUpper(par.set)

  par.types.count = getNumberOfParamTypes(par.set)

  if (n.params == 1L) {
    if (par.types %in% c("numeric", "numericvector")) {
      xs = seq(lower, upper, length.out=points.per.dim)
      ys = sapply(xs, function(x) {
        if(noisy) {
          # do replicates if noisy
          mean(replicate(noisy.evals, fun(namedList(names.x, x))))
        } else {
          fun(namedList(names.x, x))
        }
      })
      evals = data.frame(x=xs, y=ys)
    } else if (par.types %in% c("discrete")) {
      if (!noisy) {
        stopf("ExampleRun does not make sense with a single deterministic discrete parameter.")
      }

      # extract domain of discrete param
      xs = unlist(par.set$pars[[1]]$values)
      cat(xs, "\n")

      ys = sapply(xs, function(x) {
        mean(replicate(noisy.evals, fun(namedList(names.x, x))))
      })
      evals = data.frame(x=xs, y=ys)
      #print(evals)
      #stop("1d examples with discrete param are currently unter developement.")
    }
  } else if (n.params == 2L) {
    if (all(par.types %in% c("numeric", "numericvector"))) {
      eval.x = expand.grid(
        x1=seq(lower[1], upper[1], length.out=points.per.dim),
        x2=seq(lower[2], upper[2], length.out=points.per.dim)
      )
      names(eval.x) = names.x
      xs = dfRowsToList(eval.x, par.set)
      ys = sapply(xs, function(x) {
        if(noisy) {
          # do replicates if noisy
          mean(replicate(noisy.evals, fun(x)))
        } else {
          fun(x)
        }
     })
     evals = cbind(eval.x, y=ys)
    } else if (par.types.count$discrete == 1) {
      str(par.types.count)
      stop("Not implemented yet!")
    }
  }
  colnames(evals) = c(names.x, name.y)
  
  # if optimum not provided by user, estimate it
  if (is.na(global.opt))
    global.opt.estim = ifelse(control$minimize, min(evals[,name.y]), max(evals[,name.y]))
  else    
    global.opt.estim = NA_real_

  #show some info on console
  if (show.info) {
    messagef("Performing MBO on function.")
    messagef("Initial design: %i. Sequential iterations: %i.", control$init.design.points, control$iters)
    messagef("Learner: %s. Settings:\n%s", learner$id, mlr:::getHyperParsString(learner))
  }
  
  # run optimizer now
  res = mbo(fun, par.set, learner=learner, control=control, show.info=show.info)
  
  structure(list(
    par.set=par.set, n.params=n.params, par.types=par.types, 
    names.x=names.x, name.y=name.y, 
    points.per.dim=points.per.dim, evals=evals, 
    global.opt=global.opt, global.opt.estim=global.opt.estim,
    learner=learner, control=control, mbo.res=res
  ), class="MBOExampleRun")
}

#' @method print MBOExampleRun
print.MBOExampleRun = function(x, ...) {
  gap = calculateGap(as.data.frame(x$mbo.res$opt.path), x$global.opt, x$control)
  catf("MBOExampleRun")
  catf("Number of parameters        : %i", x$n.params)
  catf("Parameter names             : %s", collapse(x$names.x))
  catf("Parameter types             : %s", collapse(x$par.types))
  catf("%-28s: %.4e", getGlobalOptString(x), getGlobalOpt(x))
  catf("Gap for best point          : %.4e", gap)
  catf("True points per dim.        : %s", collapse(x$points.per.dim))
  print(x$control)
  catf("Learner                     : %s", x$learner$id)
  catf("Learner settings:\n%s", mlr:::getHyperParsString(x$learner))
  mr = x$mbo.res
  op = mr$opt.path
  catf("Recommended parameters:")
  catf(paramValueToString(op$par.set, mr$x))
  catf("Objective: %s = %.3e\n", op$y.names[1], mr$y)
}


getGlobalOpt = function(run) {
  ifelse(is.na(run$global.opt), run$global.opt.estim, run$global.opt)
}

getGlobalOptString = function(run) {
  sprintf("Global Opt (%s)",  ifelse(is.na(run$global.opt), "estimated", "known")) 
}


# FIXME: move this to ParamHelpers?
getNumberOfParamTypes = function(par.set) {
  checkArg(par.set, "ParamSet")
  supported.types = getSupportedParamTypes()
  actual.types = extractSubList(par.set$pars, "type")
  count = lapply(supported.types, function(t) {
    length(actual.types[which(actual.types == t)])
  })
  names(count) = supported.types
  return(count)
}

getSupportedParamTypes = function() {
  return(c("numeric", "integer", "numericvector", 
        "integervector", "discrete", "discretevector", "logical", 
        "logicalvector", "function", "untyped"))
}