ROC Analysis
============

The receiver operating characteristic (ROC) Curve is a graphical plot
for a binary classifier and mainly used in signal detection theory. It
compares the false positive rate (fall-out) on the horizontal axis to
the true positive rate (sensitivity) on the vertical axis by varying
the threshold. Sometimes, costs of misclassification are unknown. So,
in contrast to performance measures, where you can calculate only one
performance value, a ROC Curve for one classifier shows an area of
performance values in dependencies to different false positive and true
positive rates. Another positive aspect for this graphical plot is the
fact, that it can compare the performance of an imbalanced data set
with a modified version. The following functions are created by
including the features of the [ROCR](http://cran.r-project.org/web/packages/ROCR/index.html)-Package.

Comparing to learners
---------------------

You can easely compare the Performance of two learners with `mlr` and the help of `ROCR`.
First we create a scenario with two different [learners](learner.md) capable of predicting probabilities (Tip: run `listLearners(prob=TRUE)` to get a list of all supported learners doing so).
```{r, echo=FALSE}
library("mlbench")
library("ROCR")
library("mlr")
```

```{r}

## Generate 2 class problem with mlbench
set.seed(1)
testData = as.data.frame(mlbench.2dnormals(100, sd = 2))

## Define a learning task and an appropriate learner
task = makeClassifTask(data = testData, target = "classes")
lrn1 = makeLearner("classif.lda", predict.type = "prob")
lrn2 = makeLearner("classif.ksvm", predict.type = "prob")
```

Afterwards we perform [resampling](resample.md) to obtain predictions for each fold.
```{r}
## Perform a 10-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 10)
r1 = resample(lrn1, task, rdesc, show.info = FALSE)
r2 = resample(lrn2, task, rdesc, show.info = FALSE)
```

Now we have to convert each prediction within the resample-result to a ROCR prediction using the `asROCRPrediction()` function.
Afterwards we let `ROCR` calculate the performance measures and plot the ROC Curve.
As we have one curve for each learner and each cross validation fold we might want to average the curves from the cross validation by using `avg="threshold"`.
Otherwise `plot.performance()` will draw one curve for each fold.
For details view `?plot.performance`.

```{r ROCRaverage}
p1 = asROCRPrediction(r1$pred)
p2 = asROCRPrediction(r2$pred)
perf1 = ROCR::performance(p1,"tpr","fpr")
perf2 = ROCR::performance(p2,"tpr","fpr")
plot(perf1, col="blue", avg="threshold")
plot(perf2, col="red", avg="threshold", add=TRUE)
legend("bottomright",legend=c("lda","ksvm"), lty=1, col=c("blue","red"))
```

We can also cheat a bit and create pooled ROC-Curves by manually setting the class attribute from the prediction object from `ResamplePrediction` to `Prediction`.
```{r ROCRpooled}
r1p = r1$pred
r2p = r2$pred
class(r1p) = "Prediction"
class(r2p) = "Prediction"
p1 = asROCRPrediction(r1p)
p2 = asROCRPrediction(r2p)
perf1 = ROCR::performance(p1,"tpr","fpr")
perf2 = ROCR::performance(p2,"tpr","fpr")
plot(perf1, col="blue")
plot(perf2, col="red", add=TRUE)
```

