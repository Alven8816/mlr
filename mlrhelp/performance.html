<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<title>performance. mlr 1.2</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="author" content="">

<link href="css/bootstrap.css" rel="stylesheet">
<link href="css/bootstrap-responsive.css" rel="stylesheet">
<link href="css/highlight.css" rel="stylesheet">
<link href="css/staticdocs.css" rel="stylesheet">

<!--[if lt IE 9]>
  <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
  </head>

  <body>
    <div class="navbar">
  <div class="navbar-inner">
    <div class="container">
      <a class="brand" href="#">mlr 1.2</a>
      <div class="nav">
        <ul class="nav">
          <li><a href="index.html"><i class="icon-home icon-white"></i> Index</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>

    <div class="container">
      <header>
        
      </header>
      
      <h1>Measure performance of prediction.</h1>

<div class="row">
  <div class="span8">
    <h2>Usage</h2>
    <pre>performance(pred, measure, task, model)</pre>
    
    <h2>Arguments</h2>
    <dl>
      <dt>pred</dt>
      <dd>[<code><a href='Prediction.html'>Prediction</a></code>]  Prediction
  object to evaluate.</dd>
      <dt>measure</dt>
      <dd>[<code><a href='makeMeasure.html'>Measure</a></code>] Performance
  measure to evaluate.</dd>
      <dt>task</dt>
      <dd>[<code><a href='SupervisedTask.html'>SupervisedTask</a></code>] Learning
  task, might be requested by performance measure, usually
  not needed.</dd>
      <dt>model</dt>
      <dd>[<code><a href='makeWrappedModel.html'>WrappedModel</a></code>] Model built
  on training data, might be requested by performance
  measure, usually not needed.</dd>
    </dl>
    
    <div class="Value">
      <h2>Value</h2>
      
      <p>A single numerical performance value.</p>
  
    </div>

    <div class="Description">
      <h2>Description</h2>
      
      <p>Measures the quality of a prediction w.r.t. some
  performance measure.</p>
  
    </div>
    
    <h2 id="examples">Examples</h2>
    <pre class="examples"><div class='input'>training.set <- seq(1, nrow(iris), by = 2)
test.set <- seq(2, nrow(iris), by = 2)

task <- makeClassifTask(data = iris, target = "Species")
lrn <- makeLearner("classif.lda")
mod <- train(lrn, task, subset = training.set)
pred <- predict(mod, newdata = iris[test.set, ])

## Here we define the mean misclassification error (MMCE) as our performance measure
my.mmce <- function(task, model, pred, extra.args) {
  length(which(pred$data$response != pred$data$truth)) / nrow(pred$data)
}
ms <- makeMeasure(id = "misclassification.rate",
                  minimize = TRUE,
                  classif = TRUE,
                  allowed.pred.types = "response",
                  fun = my.mmce)
performance(pred, ms, task, mod)
</div>
<div class='output'>[1] 0,04
</div>
<div class='input'>
## Indeed the MMCE is already implemented in mlr beside other common performance measures
performance(pred, measure = mmce)
</div>
<div class='output'>[1] 0,04
</div>
<div class='input'>
## Compute multiple performance measures at once
ms <- list("mmce" = mmce, "acc" = acc, "timetrain" = timetrain)
sapply(ms, function(the.ms) {
  performance(pred, measure = the.ms, task, mod)
})
</div>
<div class='output'>     mmce       acc timetrain 
    0,040     0,960     0,006 
</div></pre>
  </div>
  <div class="span4">
    <!-- <ul>
      <li>performance</li>
    </ul>
    <ul>
      
    </ul> -->
      
    <h2>See also</h2>
    
  <code><a href='makeMeasure.html'>makeMeasure</a></code>, <code><a href='measures.html'>measures</a></code>

        
  </div>
</div>
      
      <footer>
      <p class="pull-right"><a href="#">Back to top</a></p>
<p>Built by <a href="https://github.com/hadley/staticdocs">staticdocs</a>. Styled with <a href="http://twitter.github.com/bootstrap">bootstrap</a>.</p>
      </footer>
    </div>
  </body>
</html>